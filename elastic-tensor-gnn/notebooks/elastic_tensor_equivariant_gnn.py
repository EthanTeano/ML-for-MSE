# -*- coding: utf-8 -*-
"""Elastic Tensor Equivariant GNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nUgVd-5LKnqliMyIh6R6nII_u4g_Gs2Q
"""

# --- Core PyTorch (2.2.1, CUDA 11.8 build) ---
!pip install -q torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu118

# --- PyTorch Geometric & friends (matching torch 2.2.1 + cu118) ---
!pip install -q torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric \
  --find-links https://data.pyg.org/whl/torch-2.2.1+cu118.html

# --- Materials / SciPy stack ---
!pip install -q pymatgen scipy

# --- e3nn for equivariant ops ---
!pip install -q e3nn

# ============================================================
#  CRYSTAL ELASTICITY: FAST DATA PREP + E3NN MODEL (PBC-SAFE)
#  + MatTen-style Harmonic Readout (Kelvin/BC projectors)
#  Full-dataset training (no single-structure shortcuts)
# ============================================================

# =========================
# Imports
# =========================
import os, json, math, random
import numpy as np
from typing import List, Tuple, Dict, Any, Optional
from collections import Counter
from pathlib import Path

from pymatgen.core.structure import Structure
from pymatgen.symmetry.analyzer import SpacegroupAnalyzer

import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.model_selection import StratifiedShuffleSplit

from e3nn.o3 import spherical_harmonics as e3nn_sph

# =========================
# 1) HYPERPARAMETERS (CLI)
# =========================
import argparse

def build_hparams():
    p = argparse.ArgumentParser(description="Crystal Elasticity ‚Äî E3NN + Harmonic Readout")
        # Radial basis support (can differ from neighbor cutoff if you want)
       # Radial-basis support (independent from neighbor cutoff)
    p.add_argument("--r_cut", type=float, default=6.0,
                   help="Support length (√Ö) that sets RBF frequencies; NOT the neighbor cutoff.")


    # Edge construction
    p.add_argument("--cutoff", type=float, default=4.5, help="√Ö, neighbor cutoff for graph edges")
    p.add_argument("--num_rbf", type=int, default=20,  help="Radial basis functions per edge")


    # Model size
    p.add_argument("--num_blocks",     type=int, default=3,   help="Number of InteractionBlockFull layers")
    p.add_argument("--radial_hidden",  type=int, default=128, help="Hidden width of the radial MLP in each block")
    p.add_argument("--radial_layers",  type=int, default=2,   help="Number of layers in the radial MLP (>=1)")

    # Readout + loss metric
    p.add_argument("--readout", choices=["harmonic", "voigt"], default="harmonic",
                   help="MatTen-style harmonic readout (recommended) or simple Voigt MLP")
    p.add_argument("--elastic_metric", choices=["harmonic", "kelvin", "voigt"], default="harmonic",
                   help="Space to compute the training loss")

    # Training
    p.add_argument("--batch_size",        type=int,   default=32)
    p.add_argument("--lr",                type=float, default=1e-3)
    p.add_argument("--weight_decay",      type=float, default=0.0)
    p.add_argument("--max_epochs",        type=int,   default=300)
    p.add_argument("--plateau_patience",  type=int,   default=20)
    p.add_argument("--early_patience",    type=int,   default=60)
    p.add_argument("--seed",              type=int,   default=42)
    p.add_argument("--loss_on_upper_only",action="store_true", default=True)
    p.add_argument("--amp",               action="store_true", default=False, help="Enable autocast + GradScaler")

    # Data / I/O
    p.add_argument("--filename",         type=str, default="/content/crystal_elasticity_tensor.json(2)")
    p.add_argument("--graphs_cache_dir", type=str, default="./graphs_cache")

    args, _ = p.parse_known_args()
    return args

h = build_hparams()
cutoff = h.cutoff      # neighbor search radius
r_cut  = h.r_cut       # RBF support / frequency scale (independent)
L_MAX = 4

# Basics
dtype    = torch.float32
device   = torch.device("cuda" if torch.cuda.is_available() else "cpu")
cutoff   = h.cutoff
num_rbf  = h.num_rbf
l_max    = L_MAX
filename = h.filename

graphs_cache = Path(h.graphs_cache_dir); graphs_cache.mkdir(exist_ok=True)
EDGE_DTYPE_DEFAULT = torch.float16 if torch.cuda.is_available() else torch.float32

# Repro + cudnn fast kernels
def set_repro(seed: int = 42):
    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True
set_repro(h.seed)

# =========================
# 2) I/O UTILITIES
# =========================
def find_key(obj: Any, target_key: str, path=()) -> Tuple[Optional[Any], Optional[Tuple]]:
    if isinstance(obj, dict):
        for k, v in obj.items():
            if k == target_key:
                return v, (*path, k)
            found, p = find_key(v, target_key, (*path, k))
            if p is not None:
                return found, p
    elif isinstance(obj, list):
        for i, it in enumerate(obj):
            found, p = find_key(it, target_key, (*path, i))
            if p is not None:
                return found, p
    return None, None

def to_row_from_dict(d: Dict[str, Any]) -> List[float]:
    return [d[k] for k in sorted(d.keys(), key=lambda x: int(x))]

def to_array2d_or3d(obj: Any) -> np.ndarray:
    if isinstance(obj, list):
        if len(obj) == 36 and all(not isinstance(x, (list, dict)) for x in obj):
            return np.array(obj, dtype=float).reshape(6, 6)
        return np.array(obj, dtype=float)
    if isinstance(obj, dict):
        if all(isinstance(v, dict) for v in obj.values()):
            rows = [to_row_from_dict(obj[k]) for k in sorted(obj.keys(), key=lambda x: int(x))]
            return np.array(rows, dtype=float)
        if all(isinstance(v, list) for v in obj.values()):
            rows = [obj[k] for k in sorted(obj.keys(), key=lambda x: int(x))]
            return np.array(rows, dtype=float)
        for inner in ("data", "value", "tensor", "matrix", "voigt"):
            if inner in obj:
                return to_array2d_or3d(obj[inner])
        return np.array(obj, dtype=float)
    return np.array(obj, dtype=float)

def is_structure_dict(d: dict) -> bool:
    return isinstance(d, dict) and ("lattice" in d and "sites" in d)

def extract_entries_with_labels(data: dict) -> List[Tuple[str, dict, Optional[str]]]:
    entries: List[Tuple[str, dict, Optional[str]]] = []
    if "structure" not in data and is_structure_dict(data):
        label = data.get("crystal_system")
        if label is None and isinstance(data.get("spacegroup"), dict):
            label = data["spacegroup"].get("crystal_system")
        entries.append(("root", data, label))
        return entries
    sd = data.get("structure")
    if isinstance(sd, dict):
        for k, v in sd.items():
            struct_dict = None; label = None
            if is_structure_dict(v):
                struct_dict = v
                label = v.get("crystal_system") or (v.get("spacegroup", {}) or {}).get("crystal_system")
            elif isinstance(v, dict) and "structure" in v and is_structure_dict(v["structure"]):
                struct_dict = v["structure"]
                label = v.get("crystal_system") or (v.get("spacegroup", {}) or {}).get("crystal_system")
            if struct_dict is not None:
                entries.append((str(k), struct_dict, label))
    elif isinstance(sd, list):
        for i, v in enumerate(sd):
            struct_dict = None; label = None
            if is_structure_dict(v):
                struct_dict = v
                label = v.get("crystal_system") or (v.get("spacegroup", {}) or {}).get("crystal_system")
            elif isinstance(v, dict) and "structure" in v and is_structure_dict(v["structure"]):
                struct_dict = v["structure"]
                label = v.get("crystal_system") or (v.get("spacegroup", {}) or {}).get("crystal_system")
            if struct_dict is not None:
                entries.append((f"{i}", struct_dict, label))
    return entries

# =========================
# 3) LOAD DATASET JSON (ALL SAMPLES)
# =========================

import glob, os
print("Using dataset file:", filename)

if not os.path.exists(filename):
    # Try to auto-find variants like the ‚Äú(2)‚Äù one
    base = "/content/crystal_elasticity_tensor"
    candidates = sorted(glob.glob(base + "*.json"))
    if candidates:
        print(f"‚ö†Ô∏è File not found: {filename}. Using closest match: {candidates[0]}")
        filename = candidates[0]
    # keep the original exists-check below so a real missing file still errors

if not os.path.exists(filename):
    raise FileNotFoundError(f"‚ùå File not found at: {filename}")
print(f"üìÇ Opening dataset: {filename}")
with open(filename, "r") as f:
    data = json.load(f)
print("‚úÖ JSON file loaded successfully.")

raw_voigt, voigt_path = find_key(data, "elastic_tensor_voigt")
if voigt_path is None:
    raise KeyError("No key 'elastic_tensor_voigt' found anywhere in this JSON.")
C_np = to_array2d_or3d(raw_voigt)  # (N,6,6) or (6,6)

entries_all = extract_entries_with_labels(data)
N_structs_all = len(entries_all)
if N_structs_all == 0:
    raise KeyError("No valid structures found under 'structure' or at top-level.")

if C_np.ndim == 2:
    if N_structs_all > 1:
        raise ValueError(f"Dataset has {N_structs_all} structures but only a single 6x6 tensor. Cannot train.")
    C_np = C_np.reshape(1, 6, 6)

N_targets_all = C_np.shape[0]
print(f"‚úÖ Found {N_structs_all} structure(s) and {N_targets_all} Voigt tensor(s) at path {voigt_path}.")

# Align by index; drop extras if mismatched
N = min(N_structs_all, N_targets_all)
if N < N_structs_all or N < N_targets_all:
    print(f"‚ö†Ô∏è Mismatch: using first N={N} aligned pairs; dropping {N_structs_all-N} structure(s) and {N_targets_all-N} tensor(s).")
entries = entries_all[:N]
C_np = C_np[:N]

# Labels (crystal system), compute when missing
labels: List[str] = []
missing_ids: List[str] = []
for i in range(N):
    sid_i, struct_dict_i, label_i = entries[i]
    if label_i is None:
        try:
            s_i = Structure.from_dict(struct_dict_i)
            sga = SpacegroupAnalyzer(s_i, symprec=1e-2, angle_tolerance=5)
            label_i = sga.get_crystal_system()
        except Exception:
            label_i = "unknown"; missing_ids.append(sid_i)
    labels.append(str(label_i))
labels = np.array(labels, dtype=str)
if missing_ids:
    print(f"‚ÑπÔ∏è Crystal system inferred/unknown for {len(missing_ids)} sample(s).")
print("üìä Crystal system distribution:", dict(Counter(labels)))

# =========================
# 4) EXTRACT STRUCTURES + LABELS  (full-dataset mode)
# =========================
entries = extract_entries_with_labels(data)
N_all = len(entries)
if N_all == 0:
    raise KeyError("No valid structures found under 'structure' or at top-level.")
print(f"‚úÖ Found {N_all} structure(s) in JSON.")

# Build labels (crystal system); infer missing via SpacegroupAnalyzer
labels: List[str] = []
missing_ids: List[str] = []
for sid_i, struct_dict_i, label_i in entries:
    if label_i is None:
        try:
            s_i = Structure.from_dict(struct_dict_i)
            sga = SpacegroupAnalyzer(s_i, symprec=1e-2, angle_tolerance=5)
            label_i = sga.get_crystal_system()
        except Exception:
            label_i = "unknown"
            missing_ids.append(sid_i)
    labels.append(str(label_i))
labels = np.array(labels, dtype=str)
if missing_ids:
    print(f"‚ö†Ô∏è {len(missing_ids)} structure(s) lacked a crystal_system; computed or set to 'unknown'.")

# Align targets (Voigt tensors) with entries
if C_np.ndim == 2:             # single tensor -> broadcast if exactly one entry
    if N_all != 1:
        raise ValueError("Dataset has multiple structures but only one Voigt tensor; "
                         "please provide per-structure tensors (N,6,6).")
    C_all = C_np[None, :, :]    # (1,6,6)
elif C_np.ndim == 3:
    if C_np.shape[0] != N_all:
        n = min(C_np.shape[0], N_all)
        print(f"‚ö†Ô∏è Count mismatch: {N_all} structures vs {C_np.shape[0]} tensors. "
              f"Keeping first {n} aligned samples.")
        entries = entries[:n]
        labels = labels[:n]
        C_all = C_np[:n]
    else:
        C_all = C_np
else:
    raise ValueError("Unexpected 'elastic_tensor_voigt' shape; expected (6,6) or (N,6,6).")

N = len(entries)
print(f"üì¶ Using N={N} samples after alignment.")
print("üìä Crystal system distribution (all data):", dict(Counter(labels)))


# =========================
# 5) TRAIN/VAL/TEST SPLIT (stratified & balance-checked)
# =========================
from sklearn.model_selection import StratifiedShuffleSplit
from collections import Counter

def _class_props(y, idxs, classes):
    c = Counter(y[idxs]); n = len(idxs)
    return {k: c.get(k, 0) / max(n, 1) for k in classes}

def stratified_even_split(labels, train_ratio=0.80, val_ratio=0.10, test_ratio=0.10,
                          tol=0.03, max_tries=200, seed=42):
    """
    Make train/val/test with roughly even class proportions (¬±tol of global).
    Rare classes (<3 samples) are sent entirely to train.
    """
    y = np.asarray(labels); N = len(y)
    idx_all = np.arange(N)
    counts = Counter(y)
    classes = list(counts.keys())
    global_prop = {k: counts[k] / N for k in classes}

    # Handle rare classes
    rare = {k for k, v in counts.items() if v < 3}
    rare_mask = np.isin(y, list(rare))
    rare_idx = idx_all[rare_mask]
    common_idx = idx_all[~rare_mask]
    if len(common_idx) == 0:  # everything rare -> simple seeded split
        rng = np.random.default_rng(seed)
        rng.shuffle(idx_all)
        n_tr = int(train_ratio * N); n_va = int(val_ratio * N)
        train_idx = idx_all[:n_tr]
        val_idx   = idx_all[n_tr:n_tr+n_va]
        test_idx  = idx_all[n_tr+n_va:]
        return np.sort(train_idx), np.sort(val_idx), np.sort(test_idx)

    common_y = y[common_idx]
    best = None; best_score = float("inf")
    rng = np.random.default_rng(seed)

    for _ in range(max_tries):
        rs = int(rng.integers(0, 1_000_000))

        # Train vs rest (common pool)
        sss1 = StratifiedShuffleSplit(n_splits=1, test_size=(1.0 - train_ratio), random_state=rs)
        tr_rel, rest_rel = next(sss1.split(common_idx, common_y))
        tr_c, rest_c = common_idx[tr_rel], common_idx[rest_rel]

        # Val vs Test within rest
        rest_y = y[rest_c]
        val_in_rest = val_ratio / (val_ratio + test_ratio)
        sss2 = StratifiedShuffleSplit(n_splits=1, test_size=(1.0 - val_in_rest), random_state=rs)
        va_rel, te_rel = next(sss2.split(rest_c, rest_y))
        va_c, te_c = rest_c[va_rel], rest_c[te_rel]

        # Add rare classes entirely to train
        train_idx = np.concatenate([tr_c, rare_idx])
        val_idx   = va_c
        test_idx  = te_c

        # Score deviation from global proportions
        def score(idxs):
            p = _class_props(y, idxs, classes)
            return max(abs(p[k] - global_prop[k]) for k in classes)
        score_split = max(score(train_idx), score(val_idx), score(test_idx))

        if score_split < best_score:
            best_score = score_split
            best = (np.sort(train_idx), np.sort(val_idx), np.sort(test_idx))
        if score_split <= tol:
            break

    train_idx, val_idx, test_idx = best

    # Pretty print distributions
    def summarize(name, idxs):
        c = Counter(y[idxs]); n = len(idxs)
        dist = "  ".join(f"{k}:{c.get(k,0):3d}({(c.get(k,0)/n*100 if n>0 else 0):5.1f}%)"
                         for k in sorted(classes))
        print(f"{name:5s} | n={n:4d} | {dist}")
    print("\n‚úÖ Stratified splits (‚âàeven by crystal system):")
    summarize("Train", train_idx); summarize("Val", val_idx); summarize("Test", test_idx)
    print(f"Max proportion deviation from global ‚â§ {best_score:.3f} (tolerance {tol:.3f})")

    return train_idx, val_idx, test_idx

# Build splits
train_idx, val_idx, test_idx = stratified_even_split(
    labels, train_ratio=0.80, val_ratio=0.10, test_ratio=0.10,
    tol=0.03, max_tries=200, seed=h.seed
)

# Save for reproducibility
np.savez("splits_stratified_crystal_system_from_json.npz",
         train_idx=train_idx, val_idx=val_idx, test_idx=test_idx, labels=labels)



# =========================
# 6) FEATURE Creation: NODE 1-HOT + EDGE {RBF ‚äï SH}
# =========================
try:
    from torch_geometric.data import Data
except Exception as e:
    raise RuntimeError("PyTorch Geometric is missing. In Colab, run:\n!pip -q install torch-geometric") from e

def radial_basis_function_torch(d: torch.Tensor,
                                num_rbf: int,
                                rcut: float,
                                eps: float = 1e-12) -> torch.Tensor:
    """
    Distance-only radial basis used by the edge filter MLP.
    IMPORTANT: rcut is the *radial support* (aka --r_cut), NOT the neighbor cutoff.

    RBF_n(d) = sqrt(2/rcut) * sin((n*pi/rcut) * d) / d,   n = 1..num_rbf

    Notes:
    ‚Ä¢ Well-behaved limit as d‚Üí0: sin(kd)/d ‚Üí k, so we branch to k to avoid NaNs.
    ‚Ä¢ Keep rcut independent from neighbor 'cutoff' so message spectral content
      isn't accidentally tied to graph sparsity.
    """
    device, dtype = d.device, d.dtype
    n = torch.arange(1, num_rbf + 1, device=device, dtype=dtype)     # [num_rbf]
    k = (math.pi / float(rcut)) * n                                   # [num_rbf]
    d_col = d.unsqueeze(-1)                                           # [E,1]
    kd    = d_col * k.unsqueeze(0)                                    # [E,num_rbf]
    safe  = torch.where(d_col.abs() > eps, torch.sin(kd) / d_col, k.unsqueeze(0))
    return math.sqrt(2.0 / float(rcut)) * safe                        # [E,num_rbf]

@torch.no_grad()
def build_node_features_onehot(s: Structure,
                               elem_to_idx: Dict[str, int],
                               num_elem: int) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Node features = global element one-hot (frame-invariant).
    Returns:
      x   : [N, num_elem] float32 one-hot
      pos : [N, 3] Cartesian coordinates (√Ö) float32
    """
    idxs = [elem_to_idx[site.specie.symbol] for site in s]  # global vocab
    x    = F.one_hot(torch.tensor(idxs, dtype=torch.long), num_classes=num_elem).to(torch.float32)
    pos  = torch.tensor([site.coords for site in s], dtype=torch.float32)
    return x, pos

def _pbc_neighbors_with_disps(s: Structure, cutoff: float):
    """
    Vectorized neighbor listing using pymatgen's periodic images.
    Returns: edge_src[List[int]], edge_tgt[List[int]], disp[np.ndarray, E√ó3]
    """
    edge_src, edge_tgt, disp_list = [], [], []
    for i, site in enumerate(s):
        # get_neighbors returns neighbors within 'cutoff', including periodic images.
        for nbr in s.get_neighbors(site, cutoff):
            j = int(nbr.index)
            # Skip self-edge for the central cell image
            if i == j and all(int(k) == 0 for k in getattr(nbr, "image", (0, 0, 0))):
                continue
            edge_src.append(i); edge_tgt.append(j)
            disp_list.append(nbr.coords - site.coords)
    if len(disp_list) == 0:
        return edge_src, edge_tgt, None
    return edge_src, edge_tgt, np.asarray(disp_list, dtype=np.float32)

def build_pbc_edges_and_features_fast(s: Structure,
                                      cutoff: float,
                                      num_rbf: int,
                                      r_cut: float,
                                      l_max: int,
                                      feat_device: torch.device,
                                      edge_dtype: torch.dtype
                                      ) -> Tuple[torch.Tensor, torch.Tensor, List[int]]:
    """
    Build PBC edges by 'cutoff' and compute edge attributes:
      edge_attr = [ RBF(d; r_cut)  ‚äï  SH_l<=l_max(rÃÇ) ]
    where:
      ‚Ä¢ RBF uses *r_cut* (hyperparameter controlling basis frequencies)
      ‚Ä¢ SH uses e3nn real spherical harmonics (directional encoding)

    Returns:
      edge_index: [2, E] long
      edge_attr : [E, num_rbf + sum_{l=0..l_max}(2l+1)] edge_dtype
      deg_per_node: degree list length N
    """
    edge_src, edge_tgt, disp_np = _pbc_neighbors_with_disps(s, cutoff)
    if disp_np is None:
        return (torch.empty(2, 0, dtype=torch.long, device=feat_device),
                torch.empty(0, num_rbf + (l_max + 1)**2, dtype=edge_dtype, device=feat_device),
                [0] * len(s))

      # --- Robust NumPy‚ÜíTorch conversion (handles missing NumPy C-API) ---
    try:
        disp = torch.from_numpy(disp_np).to(device=feat_device, dtype=edge_dtype)  # [E,3]
    except RuntimeError as e:
        if "Numpy is not available" in str(e):
            # Fallback: construct from Python lists (no NumPy C-API needed)
            disp = torch.tensor(disp_np.tolist(), device=feat_device, dtype=edge_dtype)
        else:
            raise

    # Distances for RBF; directions for SH
    dist = torch.linalg.norm(disp, dim=-1).to(torch.float32)  # [E]
    # --- CRITICAL FIX: use r_cut here, not 'cutoff' ---
    rbf_all = radial_basis_function_torch(dist, num_rbf=num_rbf, rcut=r_cut).to(edge_dtype).to(feat_device)

    # e3nn SH: pass full vectors; normalize=True handles |r|^l factor internally
    sh_all = e3nn_sph(list(range(l_max + 1)),
                      disp.to(torch.float32),
                      normalize=True,
                      normalization="component").to(edge_dtype).to(feat_device)  # [E, sum(2l+1)]

    edge_attr = torch.cat([rbf_all, sh_all], dim=1)  # [E, num_rbf + SH_dim]
    edge_index = torch.tensor([edge_src, edge_tgt], dtype=torch.long, device=feat_device)
    deg_per_node = torch.bincount(edge_index[0], minlength=len(s)).tolist()
    return edge_index, edge_attr, deg_per_node


# =========================
# 7) EQUIVARIANT BLOCKS
# =========================
from torch_scatter import scatter_add
from e3nn import o3
try:
    from e3nn.o3 import FullyConnectedTensorProduct
except ImportError:
    from e3nn.nn import FullyConnectedTensorProduct
from e3nn.nn import BatchNorm

def make_hidden_irreps(l_max: int) -> o3.Irreps:
    muls = {0: 32, 1: 16, 2: 8, 3: 4, 4: 2}
    parts = []
    for l in range(l_max + 1):
        m = muls.get(l, 0)
        if m > 0:
            parts.append(f"{m}x{l}{'e' if (l % 2 == 0) else 'o'}")
    return o3.Irreps(" + ".join(parts))

class RadialMLPv2(nn.Module):
    def __init__(self, in_dim: int, out_dim: int, hidden: int = 128, layers: int = 2, act=nn.SiLU):
        super().__init__()
        assert layers >= 1, "radial_layers must be >= 1"
        dims = [in_dim] + ([hidden] * (layers - 1) if layers > 1 else []) + [out_dim]
        blocks = []
        for i in range(len(dims) - 1):
            blocks.append(nn.Linear(dims[i], dims[i+1]))
            if i < len(dims) - 2:
                blocks.append(act())
        self.net = nn.Sequential(*blocks)
    def forward(self, rbf: torch.Tensor) -> torch.Tensor:
        return self.net(rbf)

class InteractionBlockFull(nn.Module):
    def __init__(self, irreps_node: o3.Irreps, irreps_sh: o3.Irreps, num_rbf: int,
                 use_batchnorm: bool = True, radial_hidden: int = 128, radial_layers: int = 2,
                 residual_weight: float = 1.0):
        super().__init__()
        self.irreps_node = o3.Irreps(irreps_node)
        self.irreps_sh   = o3.Irreps(irreps_sh)
        self.num_rbf     = int(num_rbf)
        self.res_w       = float(residual_weight)
        try:
            self.tp = FullyConnectedTensorProduct(self.irreps_node, self.irreps_sh, self.irreps_node, shared_weights=False)
        except TypeError:
            self.tp = FullyConnectedTensorProduct(self.irreps_node, self.irreps_sh, self.irreps_node)
        self.radial = RadialMLPv2(self.num_rbf, self.tp.weight_numel, hidden=radial_hidden, layers=radial_layers)
        try:
            self.self_lin = o3.Linear(self.irreps_node, self.irreps_node, bias=True)
        except TypeError:
            try:
                self.self_lin = o3.Linear(self.irreps_node, self.irreps_node, has_bias=True)
            except TypeError:
                self.self_lin = o3.Linear(self.irreps_node, self.irreps_node)
        self.bn = BatchNorm(self.irreps_node) if use_batchnorm else nn.Identity()

        # gating
        self._parts, self._scalar_slices, self._nonscalar_specs = [], [], []
        offset = 0
        for mul, ir in self.irreps_node:
            dim = mul * ir.dim; sl = slice(offset, offset + dim)
            self._parts.append((mul, ir, sl))
            if ir.l == 0: self._scalar_slices.append(sl)
            else: self._nonscalar_specs.append((mul, ir, sl))
            offset += dim
        self.scalar_dim = sum(sl.stop - sl.start for sl in self._scalar_slices)
        self.nonscalar_mul_total = sum(mul for mul, _, _ in self._nonscalar_specs)
        self.gate_mlp = None
        if self.nonscalar_mul_total > 0 and self.scalar_dim > 0:
            self.gate_mlp = nn.Sequential(
                nn.Linear(self.scalar_dim, max(32, self.scalar_dim)), nn.SiLU(),
                nn.Linear(max(32, self.scalar_dim), self.nonscalar_mul_total), nn.Sigmoid()
            )

    def forward(self, x_node: torch.Tensor, edge_index: torch.Tensor,
                edge_rbf: torch.Tensor, edge_sh: torch.Tensor) -> torch.Tensor:
        src, dst = edge_index
        w    = self.radial(edge_rbf)
        m_ij = self.tp(x_node[src], edge_sh, w)
        m_i  = scatter_add(m_ij, dst, dim=0, dim_size=x_node.shape[0])
        out  = self.self_lin(x_node) + self.res_w * m_i
        out  = self.bn(out)
        if self._nonscalar_specs:
            if self.scalar_dim > 0:
                scalars = torch.cat([out[:, sl] for sl in self._scalar_slices], dim=-1)
                gates_per_mul = self.gate_mlp(scalars)
            else:
                gates_per_mul = out.new_ones(out.size(0), self.nonscalar_mul_total)
            new_chunks, gate_ofs = [], 0
            for mul, ir, sl in self._parts:
                chunk = out[:, sl]
                if ir.l == 0:
                    new_chunks.append(F.silu(chunk))
                else:
                    g = gates_per_mul[:, gate_ofs:gate_ofs + mul].repeat_interleave(ir.dim, dim=-1)
                    new_chunks.append(g * chunk)
                    gate_ofs += mul
            out = torch.cat(new_chunks, dim=-1)
        else:
            out = F.silu(out)
        return out

# =========================
# 8) VOIGT UTILITIES
# =========================
_voigt_pairs = [(i, j) for i in range(6) for j in range(i, 6)]

def voigt6x6_to_21(C6: np.ndarray) -> np.ndarray:
    assert C6.shape == (6, 6)
    return np.array([C6[i, j] for (i, j) in _voigt_pairs], dtype=float)

def voigt21_to_6x6_torch(v21: torch.Tensor) -> torch.Tensor:
    assert v21.dim() == 2 and v21.size(1) == 21
    B = v21.size(0); C = v21.new_zeros(B, 6, 6); k = 0
    for i in range(6):
        for j in range(i, 6):
            C[:, i, j] = v21[:, k]; C[:, j, i] = v21[:, k]; k += 1
    return C

def ensure_batch_6x6(C: torch.Tensor) -> torch.Tensor:
    # Single item: (6,6) -> (1,6,6)
    if C.dim() == 2 and C.shape == (6, 6):
        return C.unsqueeze(0)

    # Common PyG-collate case: (B*6, 6) -> (B, 6, 6)
    if C.dim() == 2 and C.size(1) == 6 and C.size(0) % 6 == 0:
        B = C.size(0) // 6
        return C.reshape(B, 6, 6)

    # Already batched correctly: (B,6,6)
    if C.dim() == 3 and C.shape[1:] == (6, 6):
        return C

    raise AssertionError(f"expected (6,6) or (B,6,6), got {tuple(C.shape)}")


def upper_triangle_flat_torch(C6: torch.Tensor) -> torch.Tensor:
    C6 = ensure_batch_6x6(C6)
    B = C6.size(0); out = C6.new_empty(B, 21); k = 0
    for i in range(6):
        for j in range(i, 6):
            out[:, k] = C6[:, i, j]; k += 1
    return out

# =========================
# 9) KELVIN + HARMONIC (BC) TRANSFORMS ‚Äî rank-controlled & orthonormal
# =========================
def _kelvin_D(device=None, dtype=torch.float32):
    d = torch.tensor([1, 1, 1, math.sqrt(2), math.sqrt(2), math.sqrt(2)], dtype=dtype, device=device)
    return torch.diag(d)

def voigt_to_kelvin_6x6(C6: torch.Tensor) -> torch.Tensor:
    K = _kelvin_D(C6.device, C6.dtype)
    return K @ C6 @ K

def kelvin_to_voigt_6x6(CK: torch.Tensor) -> torch.Tensor:
    K = _kelvin_D(CK.device, CK.dtype)
    Kinv = torch.diag(1.0 / torch.diag(K))
    return Kinv @ CK @ Kinv

def _upper21_from_6x6(C: torch.Tensor) -> torch.Tensor:
    B = C.size(0); out = C.new_empty(B, 21); k = 0
    for i in range(6):
        for j in range(i, 6):
            out[:, k] = C[:, i, j]; k += 1
    return out

def _sym6_from_21(v: torch.Tensor) -> torch.Tensor:
    B = v.size(0); C = v.new_zeros(B, 6, 6); k = 0
    for i in range(6):
        for j in range(i, 6):
            C[:, i, j] = v[:, k]; C[:, j, i] = v[:, k]; k += 1
    return C

def _kelvin_ops_21(device, dtype):
    I6 = torch.eye(6, dtype=dtype, device=device)
    e = torch.zeros(6, dtype=dtype, device=device); e[:3] = 1.0 / math.sqrt(3.0)
    J = torch.outer(e, e); K = I6 - J
    kron = torch.kron

    PJJ = kron(J, J); PJK = kron(J, K); PKJ = kron(K, J); PKK = kron(K, K)

    Pmaj = torch.zeros(36, 36, dtype=dtype, device=device)
    for i in range(6):
        for j in range(6):
            Pmaj[i*6 + j, j*6 + i] = 1.0
    def sym_major(M): return 0.5 * (M + Pmaj @ M @ Pmaj)

    O_JJ = sym_major(PJJ)
    O_JK = sym_major(0.5 * (PJK + PKJ))
    O_KK = sym_major(PKK)

    G = torch.zeros(21, 36, dtype=dtype, device=device)
    S = torch.zeros(36, 21, dtype=dtype, device=device)
    lut21 = {}; k = 0
    for i in range(6):
        for j in range(i, 6):
            lut21[(i, j)] = k; k += 1
    for i in range(6):
        for j in range(6):
            if i <= j:
                k = lut21[(i, j)]
                G[k, i*6 + j] = 1.0
                S[i*6 + j, k] = 1.0 if i == j else 0.5
            else:
                k = lut21[(j, i)]
                G[k, i*6 + j] = 1.0
                S[i*6 + j, k] = 0.5
    def to21(op36): return G @ op36 @ S
    A_JJ = to21(O_JJ); A_KK = to21(O_KK); A_JK = to21(O_JK)
    A_JJ = 0.5 * (A_JJ + A_JJ.T)
    A_KK = 0.5 * (A_KK + A_KK.T)
    A_JK = 0.5 * (A_JK + A_JK.T)
    return A_JJ, A_KK, A_JK

def _orth_from_sym(A: torch.Tensor, r: int) -> torch.Tensor:
    w, V = torch.linalg.eigh(A)
    idx = torch.argsort(w, descending=True)
    U = V[:, idx[:r]]
    Q, _ = torch.linalg.qr(U, mode='reduced')
    return Q[:, :r]

def build_harmonic_transforms(device, dtype):
    A_JJ, A_KK, A_JK = _kelvin_ops_21(device, dtype)
    I21 = torch.eye(21, dtype=dtype, device=device)
    B0a = _orth_from_sym(A_JJ, r=1)
    P_res = I21 - B0a @ B0a.T
    A0b = P_res @ A_KK @ P_res
    B0b = _orth_from_sym(A0b, r=1)
    P_res = P_res - B0b @ B0b.T
    A2 = P_res @ A_JK @ P_res
    B2 = _orth_from_sym(A2, r=10)
    Ak_res = P_res @ A_KK @ P_res
    M = B2.T @ Ak_res @ B2
    wm, Vm = torch.linalg.eigh(0.5 * (M + M.T))
    idx = torch.argsort(wm, descending=True)
    U5a = B2 @ Vm[:, idx[:5]]
    U5b = B2 @ Vm[:, idx[5:10]]
    U_cat, _ = torch.linalg.qr(torch.cat([U5a, U5b], dim=1), mode='reduced')
    U5a = U_cat[:, :5]; U5b = U_cat[:, 5:10]
    P_res = P_res - U5a @ U5a.T - U5b @ U5b.T
    P_res = 0.5 * (P_res + P_res.T)
    w_res, V_res = torch.linalg.eigh(P_res); w_res = torch.clamp(w_res, min=0.0)
    P_res = (V_res * w_res) @ V_res.T
    B4 = _orth_from_sym(P_res, r=9)
    RH, _ = torch.linalg.qr(torch.cat([B0a, B0b, U5a, U5b, B4], dim=1))
    PH = RH.T
    return PH, RH

class HarmonicHead(nn.Module):
    def __init__(self, irreps_in: "o3.Irreps"):
        super().__init__()
        self.irreps_out = o3.Irreps("2x0e + 2x2e + 4e")   # 2 + 10 + 9 = 21
        self.to_irreps = o3.Linear(irreps_in, self.irreps_out)
        PH, RH = build_harmonic_transforms(device=torch.device("cpu"), dtype=torch.float32)
        self.register_buffer("PH", PH)  # Kelvin ‚Üí harmonic
        self.register_buffer("RH", RH)  # harmonic ‚Üí Kelvin
        self._slices = [slice(0,2), slice(2,12), slice(12,21)]
    def forward(self, x_graph: torch.Tensor):
        harm21 = self.to_irreps(x_graph)      # [B,21]
        kelv21 = harm21 @ self.RH.T           # assemble to Kelvin-21
        return harm21, kelv21, {
            "l0_two": harm21[:, self._slices[0]],
            "l2_two": harm21[:, self._slices[1]],
            "l4_one": harm21[:, self._slices[2]],
        }

from torch_geometric.loader import DataLoader

# === Targets for ALL entries (reuse across caching/dataloading) ===
# Assumes Section 4 produced: entries (len N), labels, and C_all with shape (N, 6, 6).

# _voigt_pairs = [(i, j) for i in range(6) for j in range(i, 6)]

# def voigt6x6_to_21(C6: np.ndarray) -> np.ndarray:
#     assert C6.shape == (6, 6)
#     return np.array([C6[i, j] for (i, j) in _voigt_pairs], dtype=float)

C_all_6x6: List[np.ndarray] = [C_all[i].astype(np.float32) for i in range(N)]
y_all_21:  List[np.ndarray] = [voigt6x6_to_21(C_all[i])    for i in range(N)]

# === Global element vocab over ALL kept structures ===
all_elements = sorted({
    site.specie.symbol
    for _, sd, _ in entries
    for site in Structure.from_dict(sd)
})
elem_to_idx_global = {el: i for i, el in enumerate(all_elements)}
num_elem_global = len(all_elements)
print(f"üåê Global element vocab ({num_elem_global}): {all_elements[:10]}{'‚Ä¶' if num_elem_global>10 else ''}")

def cache_key(idx_i: int, cutoff: float, num_rbf: int, l_max: int) -> Path:
    # include edge dtype tag to avoid mixing fp16/fp32 edge_attr in the cache
    dtype_tag = "fp16" if EDGE_DTYPE_DEFAULT == torch.float16 else "fp32"
    return graphs_cache / f"gidx{idx_i}_rc{cutoff:.2f}_rbf{num_rbf}_l{l_max}_{dtype_tag}.pt"

def build_or_load_graph(idx_i: int,
                        cutoff: float,
                        num_rbf: int,
                        l_max: int,
                        edge_dtype: torch.dtype = EDGE_DTYPE_DEFAULT) -> Optional[Data]:
    ck = cache_key(idx_i, cutoff, num_rbf, l_max)
    if ck.exists():
        return torch.load(ck, map_location="cpu")

    sid_i, struct_dict_i, _ = entries[idx_i]
    s_i = Structure.from_dict(struct_dict_i)

    # --- Node features (element one-hot + coords) ---
    x, pos = build_node_features_onehot(s_i, elem_to_idx_global, num_elem_global)
    node_elem_idx = torch.argmax(x, dim=1).to(torch.long)  # [N]  <-- FIX: now indented inside the function

    # --- Vectorized edges + features (RBF ‚äï SH) ---
    feat_device = device if torch.cuda.is_available() else torch.device("cpu")
    edge_index, edge_attr, _ = build_pbc_edges_and_features_fast(
        s_i, cutoff, num_rbf, r_cut, l_max,
        feat_device=feat_device, edge_dtype=edge_dtype
    )

    if edge_attr.numel() == 0:
        return None  # caller will try relaxed cutoffs

    # --- Assemble PyG Data object ---
    g = Data(
        x=x,
        pos=pos,
        edge_index=edge_index.cpu(),
        edge_attr=edge_attr.cpu(),
        elem_idx=node_elem_idx  # added for embedding layer
    )

    # --- Targets (Voigt 6√ó6 and Voigt-21) ---
    g.y6 = torch.tensor(C_all_6x6[idx_i], dtype=torch.float32)  # [6,6]
    g.y21 = torch.tensor(y_all_21[idx_i], dtype=torch.float32)  # [21]

    torch.save(g, ck)
    return g


from torch.utils.data import Dataset

class ElasticityDataset(Dataset):
    def __init__(self, index_array: np.ndarray):
        self.indices = list(map(int, np.asarray(index_array)))
        if len(self.indices) == 0:
            raise ValueError("Empty split passed to ElasticityDataset.")

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, k):
        idx_i = self.indices[k]
        g = build_or_load_graph(idx_i, cutoff=cutoff, num_rbf=num_rbf, l_max=l_max, edge_dtype=EDGE_DTYPE_DEFAULT)
        if g is None or g.edge_attr.numel() == 0:
            # Try to be robust: attempt a slightly larger neighbor cutoff once
            g = build_or_load_graph(idx_i, cutoff=cutoff*1.1, num_rbf=num_rbf, l_max=l_max, edge_dtype=EDGE_DTYPE_DEFAULT)

        if g is None or g.edge_attr.numel() == 0:
            raise RuntimeError(f"Graph for sample {idx_i} has no edges even after retry. Increase --cutoff.")
        return g



train_ds = ElasticityDataset(train_idx)
val_ds   = ElasticityDataset(val_idx)
test_ds  = ElasticityDataset(test_idx)

class MatTenLite(nn.Module):
    def __init__(self, num_elem: int, l_max: int, num_rbf: int,
                 hidden_irreps: Optional[o3.Irreps] = None,
                 num_blocks: int = 3, scalar_heads: int = 64,
                 use_batchnorm: bool = True,
                 radial_hidden: int = 128, radial_layers: int = 2,
                 residual_weight: float = 1.0,
                 readout: str = "harmonic"):
        super().__init__()
        self.num_rbf = num_rbf
        self.readout = readout

        self.irreps_in  = o3.Irreps(f"{num_elem}x0e")
        self.irreps_sh  = o3.Irreps.spherical_harmonics(l_max)
        self.irreps_hid = hidden_irreps or make_hidden_irreps(l_max)

        self.lift = o3.Linear(self.irreps_in, self.irreps_hid)
        self.blocks = nn.ModuleList([
            InteractionBlockFull(self.irreps_hid, self.irreps_sh, num_rbf=self.num_rbf,
                                 use_batchnorm=use_batchnorm, radial_hidden=radial_hidden,
                                 radial_layers=radial_layers, residual_weight=residual_weight)
            for _ in range(num_blocks)
        ])

        # Simple scalar readout (kept for --readout=voigt)
        self.to_scalars = o3.Linear(self.irreps_hid, o3.Irreps(f"{scalar_heads}x0e"))
        self.voigt_head = nn.Sequential(nn.Linear(scalar_heads, 64), nn.SiLU(), nn.Linear(64, 21))

        # MatTen-style harmonic readout
        self.harm_head = HarmonicHead(self.irreps_hid)

    def forward(self, batch: Data):
        # Split edge features
        edge_rbf = batch.edge_attr[:, :self.num_rbf].to(batch.x.dtype)
        edge_sh  = batch.edge_attr[:, self.num_rbf:].to(batch.x.dtype)

        # Message passing
        x = self.lift(batch.x)
        for blk in self.blocks:
            x = blk(x, batch.edge_index, edge_rbf, edge_sh)

        # Equivariant pooling (keeps l>0 content via linear equivariant map + sum)
        counts = torch.bincount(batch.batch, minlength=batch.num_graphs).clamp_min(1).view(-1, 1).to(x.device)
        x_graph = scatter_add(x, batch.batch, dim=0, dim_size=batch.num_graphs) / counts

        if self.readout == "harmonic":
            # Predict 21 harmonic coeffs; assemble to Kelvin-21; convert back to Voigt-21 for logging/metrics
            harm21, kelv21, parts = self.harm_head(x_graph)
            CK = _sym6_from_21(kelv21)        # Kelvin 6√ó6
            C  = kelvin_to_voigt_6x6(CK)      # Voigt 6√ó6
            voigt21 = upper_triangle_flat_torch(C)
            return {"harm21": harm21, "kelv21": kelv21, "voigt21": voigt21, "parts": parts}
        else:
            # Legacy scalar readout
            s = self.to_scalars(x)
            s_graph = scatter_add(s, batch.batch, dim=0, dim_size=batch.num_graphs) / counts
            voigt21 = self.voigt_head(s_graph)
            return {"harm21": None, "kelv21": None, "voigt21": voigt21, "parts": None}

# =========================
# 11) TRAINING LOOP (print error each epoch)
# =========================
def target_voigt21_from_batch(batch) -> torch.Tensor:
    C_ref = ensure_batch_6x6(batch.y6).to(batch.y6.device)
    return upper_triangle_flat_torch(C_ref)

def train_matten_style_revHD(
    train_ds, val_ds, test_ds, batch_size: int = 32, lr: float = 1e-3,
    max_epochs: int = 300, plateau_patience: int = 20, early_patience: int = 60,
    weight_decay: float = 0.0, seed: int = 42, loss_on_upper_only: bool = True,
):
    torch.manual_seed(seed); np.random.seed(seed)

    loader_kwargs = dict(batch_size=batch_size, num_workers=0, pin_memory=False, drop_last=False)
    train_loader = DataLoader(train_ds, shuffle=True,  **loader_kwargs)
    val_loader   = DataLoader(val_ds,  shuffle=False, **loader_kwargs)
    test_loader  = DataLoader(test_ds, shuffle=False, **loader_kwargs)

    model = MatTenLite(num_elem=num_elem_global, l_max=l_max, num_rbf=num_rbf,
                       num_blocks=h.num_blocks, radial_hidden=h.radial_hidden,
                       radial_layers=h.radial_layers, readout=h.readout).to(device)
    print(f"Model params: {sum(p.numel() for p in model.parameters()):,} | blocks={h.num_blocks} | radial_hidden={h.radial_hidden} | layers={h.radial_layers}")

    opt   = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode="min", factor=0.5, patience=plateau_patience)
    mse   = nn.MSELoss()
    scaler = torch.cuda.amp.GradScaler(enabled=(h.amp and torch.cuda.is_available()))

    def loss_in_metric(pred, batch):
        if h.elastic_metric == "harmonic":
            C_ref  = ensure_batch_6x6(batch.y6).to(device)
            CK_ref = voigt_to_kelvin_6x6(C_ref)
            yref21K = _upper21_from_6x6(CK_ref)
            harm_ref = yref21K @ model.harm_head.PH.T
            return mse(pred["harm21"], harm_ref)
        elif h.elastic_metric == "kelvin":
            C_ref  = ensure_batch_6x6(batch.y6).to(device)
            CK_ref = voigt_to_kelvin_6x6(C_ref)
            yref21K = upper_triangle_flat_torch(CK_ref)
            CK_hat = _sym6_from_21(pred["kelv21"])
            yhat21K = upper_triangle_flat_torch(CK_hat)
            return mse(yhat21K, yref21K)
        else:  # voigt
            voigt21_ref = target_voigt21_from_batch(batch)
            return mse(pred["voigt21"], voigt21_ref)

    @torch.no_grad()
    def quick_val_mae_voigt(loader):
        model.eval()
        total_abs, total_count = 0.0, 0
        for batch in loader:
            batch = batch.to(device)
            pred = model(batch)
            C_hat = voigt21_to_6x6_torch(pred["voigt21"])
            C_ref = ensure_batch_6x6(batch.y6)
            if loss_on_upper_only:
                diff = upper_triangle_flat_torch(C_hat) - upper_triangle_flat_torch(C_ref)
            else:
                diff = (C_hat - C_ref).reshape(C_hat.size(0), -1)
            total_abs += float(diff.abs().sum().item())
            total_count += diff.numel()
        return total_abs / max(1, total_count)

    def step(loader, train=False):
        model.train(train)
        run_loss, n_graphs = 0.0, 0
        for batch in loader:
            batch = batch.to(device)
            with torch.cuda.amp.autocast(enabled=(h.amp and torch.cuda.is_available())):
                pred = model(batch)
                loss = loss_in_metric(pred, batch)
            if train:
                opt.zero_grad(set_to_none=True)
                if scaler.is_enabled():
                    scaler.scale(loss).backward()
                    scaler.unscale_(opt)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
                    scaler.step(opt); scaler.update()
                else:
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
                    opt.step()
            run_loss += float(loss.item()) * batch.num_graphs
            n_graphs += batch.num_graphs
        return run_loss / max(n_graphs, 1)

    print("\nüöÄ Training‚Ä¶")
    best_val = float("inf"); best_state = None; no_improve = 0
    for epoch in range(1, max_epochs + 1):
        tr = step(train_loader, train=True)
        va = step(val_loader,   train=False)
        # Per-epoch error (Voigt MAE on val for human interpretability)
        val_mae_voigt = quick_val_mae_voigt(val_loader)

        sched.step(va)
        improved = va + 1e-12 < best_val
        if improved:
            best_val = va
            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}
            no_improve = 0
        else:
            no_improve += 1

        # Print error EACH epoch (train loss, val loss, val Voigt-MAE)
        print(f"Epoch {epoch:4d} | train_loss={tr:.6f} | val_loss={va:.6f} | val_MAE_Voigt={val_mae_voigt:.6e} | lr={opt.param_groups[0]['lr']:.2e}")

        if no_improve >= h.early_patience:
            print(f"‚èπÔ∏è Early stopping at epoch {epoch} (no val improvement for {h.early_patience} epochs).")
            break

    if best_state is not None:
        model.load_state_dict(best_state)
        ckpt_name = f"matten_harmonic_b{h.num_blocks}_rbf{h.num_rbf}_rh{h.radial_hidden}_rl{h.radial_layers}_rc{h.cutoff:.2f}.pt"
        torch.save(model.state_dict(), ckpt_name)
        print(f"üíæ Saved best model ‚Üí {ckpt_name}")

    # Final report on all splits in Voigt-space for readability
    @torch.no_grad()
    def eval_loader(loader):
        se6=ae6=n6=0.0; ae21=n21=0.0
        model.eval()
        for batch in loader:
            batch = batch.to(device)
            pred = model(batch)
            C_hat = voigt21_to_6x6_torch(pred["voigt21"])
            C_ref = ensure_batch_6x6(batch.y6)
            if loss_on_upper_only:
                diff = upper_triangle_flat_torch(C_hat) - upper_triangle_flat_torch(C_ref)
                se6 += float((diff**2).sum().item()); ae6 += float(diff.abs().sum().item()); n6 += diff.numel()
            else:
                diff = (C_hat - C_ref)
                se6 += float((diff**2).sum().item()); ae6 += float(diff.abs().sum().item()); n6 += diff.numel()
            ae21 += float((pred["voigt21"] - upper_triangle_flat_torch(C_ref)).abs().sum().item()); n21 += pred["voigt21"].numel()
        return se6/max(n6,1), ae6/max(n6,1), ae21/max(n21,1)

    tr_mse6, tr_mae6, tr_mae21 = eval_loader(train_loader)
    va_mse6, va_mae6, va_mae21 = eval_loader(val_loader)
    te_mse6, te_mae6, te_mae21 = eval_loader(test_loader)
    print("\nüìà Final metrics (Voigt space):")
    print(f"‚Ä¢ Train: MSE6={tr_mse6:.6e} | MAE6={tr_mae6:.6e} | MAE21={tr_mae21:.6e}")
    print(f"‚Ä¢ Val:   MSE6={va_mse6:.6e} | MAE6={va_mae6:.6e} | MAE21={va_mae21:.6e}")
    print(f"‚Ä¢ Test:  MSE6={te_mse6:.6e} | MAE6={te_mae6:.6e} | MAE21={te_mae21:.6e}")
    return model

# =========================
# 12) RUN TRAINING
# =========================
model = train_matten_style_revHD(
    train_ds, val_ds, test_ds,
    batch_size=h.batch_size, lr=h.lr,
    max_epochs=h.max_epochs, plateau_patience=h.plateau_patience, early_patience=h.early_patience,
    weight_decay=h.weight_decay, seed=h.seed,
    loss_on_upper_only=h.loss_on_upper_only,
)

#download weights
# Create two downloadable helper files for the user:
# 1) model_artifact_paths.txt ‚Äì concise reference of where training artifacts are saved
# 2) export_artifacts.py ‚Äì a self-contained script that zips model/splits/cache and writes a single zip to /content/exports/

from datetime import datetime
from pathlib import Path

base = Path("/mnt/data")
base.mkdir(parents=True, exist_ok=True)

# 1) Paths reference
paths_txt = base / "model_artifact_paths.txt"
paths_txt.write_text(
"""Crystal Elasticity Training ‚Äî Save Locations (quick reference)

1) Graph cache (per-structure PyG graphs)
   ‚Ä¢ Directory (default): ./graphs_cache   (Colab ‚Üí /content/graphs_cache)
   ‚Ä¢ Files: gidx{IDX}_rc{cutoff:.2f}_rbf{num_rbf}_l{lmax}_{fp16|fp32}.pt

2) Stratified splits (reproducibility)
   ‚Ä¢ File: ./splits_stratified_crystal_system_from_json.npz   (Colab ‚Üí /content/...)

3) Best model checkpoint (state_dict only)
   ‚Ä¢ File: ./matten_harmonic_b{num_blocks}_rbf{num_rbf}_rh{radial_hidden}_rl{radial_layers}_rc{cutoff:.2f}.pt
   ‚Ä¢ Saved when validation loss improves.

Change locations:
- Pass --graphs_cache_dir to move the cache.
- Wrap filenames with a directory, e.g. out_dir = Path('./runs/exp1'); out_dir.mkdir(parents=True, exist_ok=True);
  np.savez(out_dir / 'splits_stratified_crystal_system_from_json.npz', ...);
  torch.save(model.state_dict(), out_dir / f'matten_harmonic_b{...}.pt')
""")

# 2) Export script
export_py = base / "export_artifacts.py"
export_py.write_text(
r'''"""
Export training artifacts into a single ZIP for download (Colab- and local-friendly).

What it collects (if present):
  - Best model checkpoint: matten_harmonic_b*_rbf*_rh*_rl*_rc*.pt
  - Split file: splits_stratified_crystal_system_from_json.npz
  - Graph cache dir: graphs_cache/

Usage (Colab cell):
  !python export_artifacts.py --root /content --out /content/exports

Usage (local shell):
  python export_artifacts.py --root . --out ./exports
"""
import argparse, re, shutil
from pathlib import Path
from datetime import datetime

def find_latest_checkpoint(root: Path):
    # Matches training filename pattern (robust to params)
    pat = re.compile(r"^matten_harmonic_b\d+_rbf\d+_rh\d+_rl\d+_rc\d+\.\d{2}\.pt$")
    cands = [p for p in root.glob("*.pt") if pat.match(p.name)]
    if not cands:
        return None
    # Pick the most recently modified
    return max(cands, key=lambda p: p.stat().st_mtime)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--root", type=str, default=".", help="Project run directory (where training was executed)")
    ap.add_argument("--out",  type=str, default="./exports", help="Output directory for the ZIP")
    ap.add_argument("--include_cache", action="store_true", help="Include graphs_cache/ (can be large)")
    args = ap.parse_args()

    root = Path(args.root).resolve()
    out_dir = Path(args.out).resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    # Determine artifacts
    splits = root / "splits_stratified_crystal_system_from_json.npz"
    ckpt   = find_latest_checkpoint(root)
    cache_dir = root / "graphs_cache"

    # Prepare a staging directory
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    stage = out_dir / f"matten_export_{stamp}"
    stage.mkdir(parents=True, exist_ok=True)

    copied = []

    if ckpt and ckpt.exists():
        dst = stage / ckpt.name
        shutil.copy2(ckpt, dst)
        copied.append(dst.name)
    if splits.exists():
        dst = stage / splits.name
        shutil.copy2(splits, dst)
        copied.append(dst.name)
    if args.include_cache and cache_dir.exists():
        dst = stage / "graphs_cache"
        shutil.copytree(cache_dir, dst, dirs_exist_ok=True)
        copied.append("graphs_cache/")

    # Always include a short README with paths
    (stage / "README.txt").write_text(
        "This archive contains artifacts from your MatTen-style training run.\n"
        f"Copied: {copied}\n\n"
        "Default save locations during training:\n"
        "  - Graph cache: ./graphs_cache/\n"
        "  - Splits: ./splits_stratified_crystal_system_from_json.npz\n"
        "  - Checkpoint: ./matten_harmonic_b{...}.pt\n"
    )

    # Zip it
    zip_path = shutil.make_archive(str(stage), "zip", root_dir=stage)
    print(f"Exported ‚Üí {zip_path}")

if __name__ == "__main__":
    main()
''')

[str(paths_txt), str(export_py)]

"""üìà Final metrics (Voigt space):
‚Ä¢ Train: MSE6=1.774080e+03 | MAE6=2.120837e+01 | MAE21=2.120837e+01
‚Ä¢ Val:   MSE6=1.956601e+03 | MAE6=2.228887e+01 | MAE21=2.228887e+01
‚Ä¢ Test:  MSE6=2.172689e+03 | MAE6=2.307046e+01 | MAE21=2.307046e+01
"""